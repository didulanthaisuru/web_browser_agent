{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02d63f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\my_projects_new\\web_browser_agent\\web_agent_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import nest_asyncio\n",
    "import pprint \n",
    "import base64\n",
    "from io import BytesIO\n",
    "from playwright.async_api import async_playwright\n",
    "import google.generativeai as genai\n",
    "#import tubulate\n",
    "from PIL import Image\n",
    "from IPython.display import display, HTML,Markdown\n",
    "from pydantic import BaseModel\n",
    "from helper import get_openai_api_key,visualizeCourses,get_openai_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e62b83f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine you're teaching a dog a trick.  You show it what to do, reward it when it gets it right, and correct it when it's wrong.  Eventually, the dog learns the trick.\n",
      "\n",
      "AI is similar.  We \"teach\" computers to do things by showing them lots of examples and giving them feedback.  Instead of treats, we use data.  The computer learns patterns and rules from that data, and then uses those patterns to make decisions or predictions on its own, like recognizing faces in a photo or translating languages.\n",
      "\n",
      "It's not actually \"thinking\" like a human, but it can appear that way because it can solve problems and learn from experience.  Think of it as a really smart calculator that can learn and adapt.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client = get_openai_client()\n",
    "nest_asyncio.apply()\n",
    "\n",
    "response = client.generate_content(\"Explain AI in simple terms.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8427da",
   "metadata": {},
   "source": [
    "WebScraper Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8274a189",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraperAgent:\n",
    "    def __init__(self):\n",
    "        self.playwright = None\n",
    "        self.browser = None\n",
    "        self.page = None\n",
    "        # Apply nest_asyncio to handle async in Jupyter\n",
    "        nest_asyncio.apply()\n",
    "\n",
    "    async def init_browser(self):\n",
    "        self.playwright = await async_playwright().start()\n",
    "        self.browser = await self.playwright.chromium.launch(\n",
    "            headless=True,\n",
    "            args=[\n",
    "                \"--disable-dev-shm-usage\",\n",
    "                \"--no-sandbox\",\n",
    "                \"--disable-setuid-sandbox\",\n",
    "                \"--disable-accelerated-2d-canvas\",\n",
    "                \"--disable-gpu\",\n",
    "                \"--no-zygote\",\n",
    "                \"--disable-audio-output\",\n",
    "                \"--disable-software-rasterizer\",\n",
    "                \"--disable-webgl\",\n",
    "                \"--disable-web-security\",\n",
    "                \"--disable-features=LazyFrameLoading\",\n",
    "                \"--disable-features=IsolateOrigins\",\n",
    "                \"--disable-background-networking\"\n",
    "\n",
    "            ]\n",
    "        )\n",
    "        self.page = await self.browser.new_page()\n",
    "\n",
    "    async def scrape_content(self, url):\n",
    "        if not self.page or self.page.is_closed():\n",
    "            await self.init_browser()\n",
    "        await self.page.goto(url, wait_until=\"load\")\n",
    "        await self.page.wait_for_timeout(2000)\n",
    "        return await self.page.content()\n",
    "    \n",
    "    async def take_screenshot(self, path=\"screenshot.png\"):\n",
    "        await self.page.screenshot(path=path, full_page=True)\n",
    "        return path\n",
    "    \n",
    "    async def screenshot_buffer(self):\n",
    "        screenshot_bytes = await self.page.screenshot(type=\"png\", full_page=False)\n",
    "        return screenshot_bytes\n",
    "    \n",
    "    async def close(self):\n",
    "        try:\n",
    "            if self.browser:\n",
    "                await self.browser.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error closing browser: {e}\")\n",
    "        \n",
    "        try:\n",
    "            if self.playwright:\n",
    "                await self.playwright.stop()\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error stopping playwright: {e}\")\n",
    "        \n",
    "        self.playwright = None\n",
    "        self.browser = None\n",
    "        self.page = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80d6ada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = WebScraperAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d17324",
   "metadata": {},
   "source": [
    "Structured data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a86c5665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeplearningCourse(BaseModel):\n",
    "    title: str\n",
    "    description: str\n",
    "    presenter: list[str]\n",
    "    imageUrl:str\n",
    "    courseURL: str\n",
    "\n",
    "class DeeplearningCourseList(BaseModel):\n",
    "    courses: list[DeeplearningCourse]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722abe90",
   "metadata": {},
   "source": [
    "LLM Client for Gemini Ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebd099aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_with_llm(html, instructions, truncate=False):\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert web scraping agent. Your task is to:\n",
    "    Extract relevant information from this HTML to JSON \n",
    "    following these instructions:\n",
    "    {instructions}\n",
    "    \n",
    "    Extract the title, description, presenter, \n",
    "    the image URL and course URL for each of \n",
    "    all the courses for the deeplearning.ai website\n",
    "\n",
    "    Return ONLY valid JSON in the following format:\n",
    "    {{\n",
    "        \"courses\": [\n",
    "            {{\n",
    "                \"title\": \"Course Title\",\n",
    "                \"description\": \"Course Description\", \n",
    "                \"presenter\": [\"Presenter Name\"],\n",
    "                \"imageUrl\": \"Image URL\",\n",
    "                \"courseURL\": \"Course URL\"\n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "\n",
    "    HTML Content:\n",
    "    {html[:150000]}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.generate_content(prompt)\n",
    "    \n",
    "    # Parse the JSON response\n",
    "    import json\n",
    "    try:\n",
    "        json_text = response.text.strip()\n",
    "        # Remove markdown code blocks if present\n",
    "        if json_text.startswith('```json'):\n",
    "            json_text = json_text[7:]\n",
    "        if json_text.endswith('```'):\n",
    "            json_text = json_text[:-3]\n",
    "        json_text = json_text.strip()\n",
    "        \n",
    "        parsed_data = json.loads(json_text)\n",
    "        return DeeplearningCourseList(**parsed_data)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON parsing error: {e}\")\n",
    "        print(f\"Response text: {response.text[:500]}...\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fcfb6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def webscraper(target_url, instructions):\n",
    "    result = None\n",
    "    screenshot = None\n",
    "    try:\n",
    "        print(\"Extracting HTML Content\\n\")\n",
    "        html_content = await scraper.scrape_content(target_url)\n",
    "\n",
    "        print(\"Taking Screenshot \\n\")\n",
    "        screenshot = await scraper.screenshot_buffer()\n",
    "\n",
    "        print(\"Processing..\")\n",
    "        result: DeeplearningCourseList = await process_with_llm(html_content, instructions, False)\n",
    "        if result:\n",
    "            print(\"\\nGenerated Structured Response\")\n",
    "        else:\n",
    "            print(\"\\nFailed to generate structured response\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "    finally:\n",
    "        await scraper.close()\n",
    "    return result, screenshot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81dd337",
   "metadata": {},
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "046bdf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_url=\"https://www.deeplearning.ai/courses\"\n",
    "base_url=\"https://www.deeplearning.ai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae32bade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting HTML Content\n",
      "\n",
      "Error: \n"
     ]
    }
   ],
   "source": [
    "instructions=\"\"\"Get all the courses\"\"\"\n",
    "result,screenshot = await webscraper(target_url,instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "443b7597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting HTML Content with requests...\n",
      "HTML extracted, length: 106065\n",
      "Processing with LLM...\n",
      "\n",
      "Generated Structured Response\n",
      "\n",
      "Found 7 courses!\n",
      "\n",
      "1. AI Python for Beginners\n",
      "   Presenter: ['DeepLearning.AI']\n",
      "   URL: /courses/ai-python-for-beginners\n",
      "\n",
      "2. ChatGPT Prompt Engineering for Developers\n",
      "   Presenter: ['OpenAI']\n",
      "   URL: /courses/chatgpt-prompt-engineering-for-developers\n",
      "\n",
      "3. Generative AI for Everyone\n",
      "   Presenter: ['DeepLearning.AI']\n",
      "   URL: /courses/generative-ai-for-everyone\n"
     ]
    }
   ],
   "source": [
    "# Fix for Windows asyncio issues with Playwright\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "if platform.system() == 'Windows':\n",
    "    # Set the event loop policy for Windows\n",
    "    if sys.version_info >= (3, 8):\n",
    "        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
    "\n",
    "# Alternative: Let's try using requests and BeautifulSoup instead of Playwright for now\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "async def simple_webscraper(target_url, instructions):\n",
    "    result = None\n",
    "    screenshot = None\n",
    "    try:\n",
    "        print(\"Extracting HTML Content with requests...\")\n",
    "        \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(target_url, headers=headers)\n",
    "        html_content = response.text\n",
    "        \n",
    "        print(f\"HTML extracted, length: {len(html_content)}\")\n",
    "        print(\"Processing with LLM...\")\n",
    "        \n",
    "        result: DeeplearningCourseList = await process_with_llm(html_content, instructions, False)\n",
    "        if result:\n",
    "            print(\"\\nGenerated Structured Response\")\n",
    "        else:\n",
    "            print(\"\\nFailed to generate structured response\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return result, screenshot\n",
    "\n",
    "# Test the simple scraper\n",
    "result, screenshot = await simple_webscraper(target_url, instructions)\n",
    "if result:\n",
    "    print(f\"\\nFound {len(result.courses)} courses!\")\n",
    "    for i, course in enumerate(result.courses[:3]):  # Show first 3 courses\n",
    "        print(f\"\\n{i+1}. {course.title}\")\n",
    "        print(f\"   Presenter: {course.presenter}\")\n",
    "        print(f\"   URL: {course.courseURL}\")\n",
    "else:\n",
    "    print(\"No courses found or parsing failed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_agent_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
